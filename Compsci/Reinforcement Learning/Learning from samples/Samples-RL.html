<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Value-based RL II</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Dark theme (embedded) -->
  <style>
    :root{
      --bg:#0f172a;      /* slate-900 */
      --bg2:#111827;     /* slate-800 */
      --text:#e2e8f0;    /* slate-200 */
      --muted:#94a3b8;   /* slate-400 */
      --accent:#38bdf8;  /* sky-400 */
      --code:#1f2937;    /* slate-700 */
      --radius:16px;
      --maxw:900px;
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0 auto !important;
      max-width:var(--maxw);
      background:var(--bg);
      color:var(--text);
      font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",sans-serif;
      line-height:1.6;
      padding:0 1rem 3rem;
      display:block !important; /* defeat any theme flex */
    }
    header{padding:2.2rem 0 1.1rem}
    h1{font-size:2.2rem;margin:.1rem 0 .25rem}
    h2{margin:2rem 0 1rem;border-bottom:1px solid rgba(148,163,184,.25);padding-bottom:.25rem}
    p,li{color:var(--text)}
    em{color:var(--accent);font-style:normal}
    a{color:var(--accent)}
    ul,ol{padding-left:1.25rem;margin:.25rem 0 1rem}
    li{margin:.35rem 0}
    figure{background:var(--bg2);padding:1rem;border-radius:var(--radius);text-align:center;margin:1.25rem 0}
    figure img{max-width:100%;height:auto;border-radius:12px}
    figcaption{color:var(--muted);font-size:.9rem;margin-top:.5rem}
    code,pre{
      background:var(--code);border-radius:.4rem;padding:.15rem .4rem;
      font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;
      font-size:.9rem;
    }
    pre{padding:1rem;overflow:auto}

    /* MathJax tweaks */
    .MathJax_Display{ text-align:left !important; } /* left-align block math */
    mjx-container[jax="CHTML"]{ line-height:1; }

    @media (max-width:700px){
      body{max-width:100%}
      h1{font-size:1.9rem}
    }
  </style>

  <!-- MathJax: inline \( \) / $ $ ; display \[ \] / $$ $$ -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['\\[', '\\]'], ['$$','$$']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
</head>
<body>
  <header>
    <h1>Value-based RL II</h1>
  </header>

  <main>
    <section>
      <h2>1 Introduction</h2>
      <p>
        The value-based methods previously presented all presuppose a known world model (i.e., the transition probability
        distribution is known). However, in most RL applications—especially in the real world—the world model isn’t known;
        therefore we rely on estimation by having the agent interact with the environment and using the sampled data from
        those interactions.
      </p>
      <p>
        There are two well-tested methods used in this regard:
        <strong>Monte Carlo</strong> control and <strong>Temporal Difference learning</strong>.
      </p>
    </section>

    <section>
      <h2>2 Monte Carlo control</h2>
      <p>Monte Carlo control consists of two steps:</p>
      <ul>
        <li>
          <strong>Estimation:</strong> we roll out a policy from an initial state (or from an initial state–action pair if
          we’re estimating the Q-function), collect rewards, and—once the discount factor makes later rewards negligible
          or the episode ends—we update our estimate of the value function using the average discounted return:
          <br/><br/>
          \[
          V(s_t) \leftarrow V(s_t) + \eta \bigl[ G_t - V(s_t) \bigr].
          \]
        </li>
        <li>
          <strong>Improvement:</strong> once we have an estimate, we use policy iteration to learn an optimal policy by making
          a greedy selection on update:
          <br/><br/>
          \[
          \pi_{k+1}(s) = \arg\max_a Q_k(s,a),
          \]
          with \( Q_k(s,a) \) approximated with Monte Carlo estimation.
        </li>
      </ul>
    </section>

    <section>
      <h2>3 Temporal Difference (TD) Learning</h2>
      <p>
        Temporal Difference learning reduces Bellman error for sampled states (or state–action pairs) during transitions:
      </p>
      <p>
        \[
        V(s_t) \leftarrow V(s_t) + \eta \left[ r_t + \gamma V(s_{t+1}) - V(s_t) \right],
        \]
      </p>
      <p>
        where \( V(s_t) \) is moved toward the target value \( r_t + \gamma V(s_{t+1}) \).
      </p>
    </section>

    <section>
      <h2>4 Combining Monte Carlo and TD</h2>
      <p>
        Monte Carlo and TD are often combined. TD updates at each transition, whereas MC waits until an episode finishes
        (or the horizon is effectively reached). We can keep the TD estimation at each transition and, for a chosen \( n \),
        approximate the remainder of the trajectory with a weighted average of \( n \)-step returns parameterized by
        \( \lambda \in [0,1] \):
      </p>
      <p>
        \[
        G_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+n}.
        \]
      </p>
      <p>
        This is the <strong>lambda-return</strong>, which we use in place of \( G_{t:t+n} \) for subsequent TD updates.
        Notable cases:
      </p>
      <ul>
        <li>\( \lambda = 1 \): the lambda-return is the standard MC return.</li>
        <li>\( \lambda = 0 \): the lambda-return is the standard TD(0) return; combining MC and TD this way is termed <strong>TD(\(\lambda\))</strong>.</li>
      </ul>
      <p>
        TD policy learning can be done <em>on-policy</em> (<strong>SARSA</strong>) or <em>off-policy</em>
        (<strong>Q-learning</strong>). We’ll expand on both and provide simple implementations in the next article.
      </p>
    </section>
  </main>
</body>
</html>
